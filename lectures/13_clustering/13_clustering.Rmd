---
title: "ISA 419: Data-Driven Security"
subtitle: "13: Clustering"
author: '<br>Fadel M. Megahed, PhD <br><br> Endres Associate Professor <br> Farmer School of Business<br> Miami University<br><br> [`r icons::icon_style(icons::fontawesome("twitter"), fill = "white")` @FadelMegahed](https://twitter.com/FadelMegahed) <br> [`r icons::icon_style(icons::fontawesome("github"), fill = "white")` fmegahed](https://github.com/fmegahed/) <br> [`r icons::icon_style(icons::fontawesome("paper-plane", style = "solid"), fill = "white")` fmegahed@miamioh.edu](mailto:fmegahed@miamioh.edu)<br> [`r icons::icon_style(icons::fontawesome("question"), fill = "white")` Automated Scheduler for Office Hours](https://calendly.com/fmegahed)<br><br>'
date: "Spring 2024"
output:
  xaringan::moon_reader:
    self_contained: true
    css: [default, "../../style_files/fonts.css", "../../style_files/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightSpans: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: "16:9"
header-includes:  
  - "../../style_files/header.html"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  progress = FALSE, 
  verbose = FALSE,
  dev = 'png',
  fig.height = 3,
  dpi = 300,
  fig.align = 'center')

options(htmltools.dir.version = FALSE)


miamired = '#C3142D'

if(require(pacman)==FALSE) install.packages("pacman")
if(require(devtools)==FALSE) install.packages("devtools")
if(require(countdown)==FALSE) devtools::install_github("gadenbuie/countdown")
if(require(xaringanExtra)==FALSE) devtools::install_github("gadenbuie/xaringanExtra")
if(require(urbnmapr)==FALSE) devtools::install_github('UrbanInstitute/urbnmapr')
if(require(emo)==FALSE) devtools::install_github("hadley/emo")

knitr::knit_engines$set(python = reticulate::eng_python)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
if(require(xaringanthemer) == FALSE) install.packages("xaringanthemer")
library(xaringanthemer)

style_mono_accent(base_color = "#84d6d3",
        base_font_size = "20px")

xaringanExtra::use_xaringan_extra(c("tile_view", "tachyons", "panelset", "search", "fit_screen", "editable", "clipboard"))

xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,
  mute_unhighlighted_code = TRUE
)
```


## Quick Refresher of Last Class

`r emo::ji("check")` Explain the difference between supervised and unsupervised learning.    

`r emo::ji("check")` EUnderstand the importance of different preprocessing steps and when they should be used in ML.  

`r emo::ji("check")` Explain typical error measures used for supervised and unsupervised learning tasks.


---

## Learning Objectives for Today's Class

- Define clustering   

- Explain the $k-$means clustering algorithm for numeric datasets  

- Implement the $k-$means algorithm in Python using the `pycaret` package  

- Describe scenarios where other/advanced clustering algorithms are needed



---

class: inverse, center, middle

# What is Clustering?


---

## The Problem of Clustering

- Given a **set of (high-dimensional) observations**, with a notion of **distance** between observations, **group the observations** into **some number of clusters**, so that:   
  +  Members of a cluster are close/similar to each other  
  +  Members of different clusters are dissimilar  

- **Usually:**  
  + The observations are in a high-dimensional space  
  + Similarity is defined using a distance measure, e.g., 
      * Euclidean, Cosine, Jaccard, edit distance, etc.
      
.footnote[
<html>
<hr>
</html>
**Source:** J. Leskovec, A. Rajaraman, J. Ullman: Mining of Massive Datasets, <http://www.mmds.org>
]



---

## Clustering in 2D Space: A Fun Example

.pull-left[
.center[

**Meet the Palmer penguins**

[![](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png)](https://allisonhorst.github.io/palmerpenguins/)
]
]

.pull-right[
.center[

**Anatomical description of the dataset:**

[![](https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png)](https://allisonhorst.github.io/palmerpenguins/)
]
]

.footnote[
<html>
<hr>
</html>

**Source:** The data are available by [CC-0 license](https://creativecommons.org/share-your-work/public-domain/cc0/) in accordance with the [Palmer Station LTER Data Policy and the LTER Data Access Policy for Type I data](http://pal.lternet.edu/data/policies). The artwork is by Allison Horst and available at <https://allisonhorst.github.io/palmerpenguins/>, and the data is downloaded using the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) `r fontawesome::fa('r-project', miamired)` package.
]


---

## Clustering in 2D Space: Formulation

- Given a **set of observations (each containing bill length and depth)**, with a notion of **Euclidean distance** between observations, **group the observations** into **3 clusters**, so that:  

  +  Members of a cluster are close/similar to each other  
  +  Members of different clusters are dissimilar  
  
- Note we are assuming that we did not have a "label/type" for each penguin.

.footnote[
<html>
<hr>
</html>

**Source:** The data are available by [CC-0 license](https://creativecommons.org/share-your-work/public-domain/cc0/) in accordance with the [Palmer Station LTER Data Policy and the LTER Data Access Policy for Type I data](http://pal.lternet.edu/data/policies). The artwork is by Allison Horst and available at <https://allisonhorst.github.io/palmerpenguins/>, and the data is downloaded using the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) `r fontawesome::fa('r-project', miamired)` package.
]


---

## Clustering in 2D Space: Raw Data

```{r bill_length_depth1, echo=FALSE}
library(palmerpenguins)
library(tidyverse)
library(factoextra)
library(NbClust)

ggplot(data = penguins,
       aes(x = bill_length_mm,
           y = bill_depth_mm,
           group = species)) +
  geom_point(size = 3,
             alpha = 0.8) +
  theme_minimal() +
  labs(title = "Penguin bill dimensions",
       subtitle = "Bill length and depth for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       color = "Penguin species",
       shape = "Penguin species") +
  theme(legend.position = c(0.85, 0.15),
        legend.background = element_rect(fill = "white", color = NA),
        plot.title.position = "plot",
        plot.caption = element_text(hjust = 0, face= "italic"),
        plot.caption.position = "plot")
```

.footnote[
<html>
<hr>
</html>

**Source:** The data are available by [CC-0 license](https://creativecommons.org/share-your-work/public-domain/cc0/) in accordance with the [Palmer Station LTER Data Policy and the LTER Data Access Policy for Type I data](http://pal.lternet.edu/data/policies). The artwork is by Allison Horst and available at <https://allisonhorst.github.io/palmerpenguins/>, and the data is downloaded using the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) `r fontawesome::fa('r-project', miamired)` package.
]

---

## Clustering in 2D Space: Labeled Raw Data

```{r bill_length_depth2, echo=FALSE}
ggplot(data = penguins,
       aes(x = bill_length_mm,
           y = bill_depth_mm,
           group = species)) +
  geom_point(aes(color = species,
                 shape = species),
             size = 3,
             alpha = 0.8) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  labs(title = "Penguin bill dimensions",
       subtitle = "Bill length and depth for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER",
       x = "Bill length (mm)",
       y = "Bill depth (mm)",
       color = "Penguin species",
       shape = "Penguin species") +
  theme(legend.position = c(0.85, 0.15),
        legend.background = element_rect(fill = "white", color = NA),
        plot.title.position = "plot",
        plot.caption = element_text(hjust = 0, face= "italic"),
        plot.caption.position = "plot")
```

.footnote[
<html>
<hr>
</html>

**Source:** The data are available by [CC-0 license](https://creativecommons.org/share-your-work/public-domain/cc0/) in accordance with the [Palmer Station LTER Data Policy and the LTER Data Access Policy for Type I data](http://pal.lternet.edu/data/policies). The artwork is by Allison Horst and available at <https://allisonhorst.github.io/palmerpenguins/>, and the data is downloaded using the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) `r fontawesome::fa('r-project', miamired)` package.
]

---

## Clustering in 2D Space: Clustering Results

```{r bill_length_depth3, echo=FALSE}
set.seed(2022)
penguins_subset = penguins |> 
  dplyr::select(species, bill_length_mm, bill_depth_mm) |> 
  dplyr::group_by(species) |> 
  na.omit() |> 
  dplyr::slice_sample(n = 20) |> 
  dplyr::ungroup() |> 
  data.frame()

row.names(penguins_subset) = paste0( stringr::str_sub(penguins_subset$species, 1, 1),
                                     1:nrow(penguins_subset) )

penguins_subset = na.omit(penguins_subset) |> dplyr::select(-species)

km_results = kmeans(penguins_subset,
                    centers = 3)

fviz_cluster(km_results, data = penguins_subset,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal()) +
  labs(title = "Clustering of a sample of 60 penguins into three groups",
       subtitle = "Using only bill length and depth",
       x = "Bill length (mm)",
       y = "Bill depth (mm)") +
  theme(legend.position = 'none',
        plot.title.position = "plot",
        plot.caption = element_text(hjust = 0, face= "italic"),
        plot.caption.position = "plot")
```

.footnote[
<html>
<hr>
</html>

**Source:** The data are available by [CC-0 license](https://creativecommons.org/share-your-work/public-domain/cc0/) in accordance with the [Palmer Station LTER Data Policy and the LTER Data Access Policy for Type I data](http://pal.lternet.edu/data/policies). The artwork is by Allison Horst and available at <https://allisonhorst.github.io/palmerpenguins/>, and the data is downloaded using the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) `r fontawesome::fa('r-project', miamired)` package.
]


---

## Comments on the 2D Clustering Problem

Even though the 2D Space clustering problem is the easiest problem to "solve" since we can benefit by plotting the data, **clustering is hard**. 

**Some important questions:**

  1. With all the variables being numerical, we often assume **Euclidean distance**. This can be problematic when:  
    - variables have significantly different scales  
    - we are including information that is not pertinent to grouping
  
  2. How do you determine the number of clusters (*k*)?  
  
  3. How to represent a cluster of many points?  
  
  4. How do we determine the "nearness" of clusters?
  

---

## An Overview of Clustering Methods

[![](https://csdl-images.ieeecomputer.org/trans/ec/2014/03/figures/fahad.t1-2330519.gif)](https://www.computer.org/csdl/journal/ec/2014/03/06832486/13rRUEgs2xB)

.footnote[
<html>
<hr>
</html>

**Source:** A. Fahad, et al.,"A Survey of Clustering Algorithms for Big Data: Taxonomy and Empirical Analysis" in IEEE Transactions on Emerging Topics in Computing, vol. 2, no. 03, pp. 267-279, 2014. <https://doi.org/10.1109/TETC.2014.2330519>
]


---
class: inverse, center, middle

# $k-$Means Clustering

---

## The General Idea

The $k$-means algorithm clusters data by trying to separate samples in $n$ groups of equal variance, minimizing a criterion known as the **inertia** or **within-cluster sum-of-squares** (see below). This algorithm requires the **number of clusters to be specified**. 

.center[
$\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)$
]


**Inertia is a measure of how internally coherent clusters are; however, it suffers from various drawbacks:**

- Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.   

- Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated.


.footnote[
<html>
<hr>
</html>

**Source:** Clustering &mdash; scikit-learn 1.0.2 documentation <https://scikit-learn.org/stable/modules/clustering.html#$k$-means>
]


---

## The Steps of the $k$-means Algorithm

 In basic terms, the algorithm has three steps. 
 
 0. Step 0 chooses the initial centroids, with the most basic method being to choose $k$samples from the dataset $X$ . After initialization, $k$-means consists of looping between the remaining two steps.  
 
 1. Step 1 assigns each sample to its nearest centroid. 
 
 2. Step 2 creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed.
 
 **The algorithm repeats these last two steps the centroids do not move significantly.**


.footnote[
<html>
<hr>
</html>

**Source:** Clustering &mdash; scikit-learn 1.0.2 documentation <https://scikit-learn.org/stable/modules/clustering.html#$k$-means>
]


---

## The $k$-means Algorithm: A Visual Example

```{r kmeans1, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://dashee87.github.io/images/kmeans.gif")
```

.footnote[
<html>
<hr>
</html>

**Source:** David Sheehan. (2018). [Clustering with Scikit with GIFs. ](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/) 
]


---

##  A Demonstration of $k$-means Clustering

Use the $k$-means algorithm to cluster the following observations. Use $k=2$ and Euclidean distance. **Use [this handout](https://miamioh.instructure.com/courses/209406/files/31265757?module_item_id=5085021) to go through the $k$-means algorithm implementation (by hand).**

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-amwm">Observation</th>
    <th class="tg-amwm">X1</th>
    <th class="tg-amwm">X2</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-baqh">1</td>
    <td class="tg-0lax">1.0</td>
    <td class="tg-0lax">1.0</td>
  </tr>
  <tr>
    <td class="tg-baqh">2</td>
    <td class="tg-0lax">1.5</td>
    <td class="tg-0lax">2.0</td>
  </tr>
  <tr>
    <td class="tg-baqh">3</td>
    <td class="tg-0lax">3.0</td>
    <td class="tg-0lax">4.0</td>
  </tr>
  <tr>
    <td class="tg-baqh">4</td>
    <td class="tg-0lax">5.0</td>
    <td class="tg-0lax">7.0</td>
  </tr>
  <tr>
    <td class="tg-baqh">5</td>
    <td class="tg-0lax">3.5</td>
    <td class="tg-0lax">5.0</td>
  </tr>
  <tr>
    <td class="tg-baqh">6</td>
    <td class="tg-0lax">4.5</td>
    <td class="tg-0lax">5.0</td>
  </tr>
  <tr>
    <td class="tg-baqh">7</td>
    <td class="tg-0lax">3.5</td>
    <td class="tg-0lax">4.5</td>
  </tr>
</tbody>
</table>



---

# Practical Issues with $k$-means Clustering

.panelset[
.panel[.panel-name[Data]


```{r penguins1, echo=FALSE}
penguins_tbl = palmerpenguins::penguins # our data for today

penguins_tbl # printing it out
```
]

.panel[.panel-name[Prep]

```{r penguins2, echo=FALSE}
penguins_tbl = penguins_tbl |>  
  # selecting relevant cols
  dplyr::select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |> 
  na.omit() |> # removing NAs
  dplyr::mutate_at(dplyr::vars(-species), scale) # scaling numeric variables

penguins_tbl # printing it out
```


]

.panel[.panel-name[k-means (k=3)]

```{r penguins3, echo=FALSE}
km_res = kmeans(
  x = penguins_tbl |> 
    dplyr::select(-species), # input data with no label
  centers = 3) # k =3

# tabulating the results with rows corresponding to true labels and the columns corresponding to cluster
table(penguins_tbl$species, km_res$cluster)
```

]

.panel[.panel-name[Optimal $k$]

```{r, penguins4, fig.show='hide', echo=FALSE}
km_res_nbclust = NbClust::NbClust(
  data = penguins_tbl |> dplyr::select(-species),
  distance = "euclidean",
  min.nc = 2, max.nc = 10, 
  method = "kmeans", index ="all") 

table(penguins_tbl$species, km_res_nbclust$Best.partition)
```


]

.panel[.panel-name[Viz Clusters]

```{r penguins6, echo=FALSE}
factoextra::fviz_cluster(
  object = 
    list(
      cluster = km_res_nbclust$Best.partition, 
      data = penguins_tbl |> dplyr::select(-species) 
    ),
  ellipse.type = "convex",
  palette = "jco",
  ggtheme = theme_minimal()
)
```
]
]


---

## Summary of Practical Issues

- Rescale numeric data prior to $k$-means implementation. The scaling can be:  
  + a z-transformation similar to what we did in the example  
  + a 0-1 scaling 
  + converting count data into percentage or counts per a certain number of the population  
  + etc.

- Use more than one metric to determine $k$ when using $k$-means clustering  

- Your cluster solution is not the end result, you will need to:  
  + visualize it in appropriate way (simple representation as in the previous slide, [spatially](https://fmegahed.github.io/covid_analysis_final.html#33_Visualizing_the_Clustering_Results), [time-based](https://fmegahed.github.io/isa401/class23/23_data_mining_overview.html?panelset1=calendar-plot-of-clustered-data&panelset4=data3&panelset5=data4&panelset6=activity3&panelset7=activity4#10), etc.)  
  + Attempt to explain the cluster membership using an appropriate binomial/multinomial model (e.g., see [this analysis](https://fmegahed.github.io/covid_analysis_final.html#4_Explanatory_Modeling_of_Cluster_Assignments))


---
class: inverse, center, middle

# Clustering Implementation in Python

---

## The `pycaret` Package

```{python pycaret, eval=FALSE}
import pandas as pd
from pycaret.clustering import *
from janitor import clean_names

portmap_df = (
    pd.read_csv('https://raw.githubusercontent.com/fmegahed/isa419/main/data/portmap.csv')
    .sample(n = 1000, random_state = 123)
    .clean_names()
    .dropna()
    .reset_index(drop = True)
    .loc[:, ['_flow_duration',	'_total_fwd_packets',	'_total_backward_packets',	'total_length_of_fwd_packets',	'_total_length_of_bwd_packets',	'_fwd_packet_length_max', '_label']]
)

s = setup(
    portmap_df[subset_features], session_id = 2024, 
    ignore_features= ['_label'], 
    preprocess=True,
    normalize = True, normalize_method= 'zscore')

kmeans = create_model('kmeans', num_clusters = 2)
kmeans_results = assign_model(kmeans)

plot_model(kmeans, plot = 'elbow')
plot_model(kmeans, plot = 'cluster', feature = '_label')
```
]


---
class: inverse, center, middle

# Other Clustering Methods

---

## Other Clustering Methods

- **Hierarchical Clustering**: This method does not require the number of clusters to be specified. It is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:  
  + **Agglomerative**: This is a "bottom-up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.  
  + **Divisive**: This is a "top-down" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.  
  
- **DBSCAN**: This is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions.  

---

## Other Clustering Methods

- **Memory-based Clustering**: This is a family of clustering algorithms that do not require the entire dataset to be in memory. Examples include:  
  + **BIRCH**: This is a hierarchical clustering algorithm that can be used to cluster very large datasets.  
  + **CLARANS**: This is a clustering algorithm that can be used to cluster very large datasets.


---
class: inverse, center, middle

# Recap

---

## Summary of Main Points

By now, you should be able to do the following:  

- Define clustering   

- Explain the $k-$means clustering algorithm for numeric datasets  

- Implement the $k-$means algorithm in Python using the `pycaret` package  

- Describe scenarios where other/advanced clustering algorithms are needed 

---

## 📝 Review and Clarification 📝

1. **Class Notes**: Take some time to revisit your class notes for key insights and concepts.
2. **Zoom Recording**: The recording of today's class will be made available on Canvas approximately 3-4 hours after the end of class.
3. **Questions**: Please don't hesitate to ask for clarification on any topics discussed in class. It's crucial not to let questions accumulate. 

